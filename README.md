A neural network from scratch! 
- Implements linear layers, activation functions like ReLU, and matrix operations all from scratch. 
- We also implement backpropagation from scratch (using the same backward/forward ideas of PyTorch). Partial gradients are computed from scratch.

TODOs
- handle multi-batches
- enable parallel computation on GPUs
- train on simple task like MNIST

